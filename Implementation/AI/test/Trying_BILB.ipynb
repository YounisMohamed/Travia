{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5e73db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Create the pipeline with BLIP image captioning model\n",
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f3c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70b364b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'arafed man with a muscular body sitting in a chair'}]\n"
     ]
    }
   ],
   "source": [
    "# Load your image from your local path\n",
    "image_path = r\"C:\\Users\\AbdulRahman\\Desktop\\backup\\david_laid.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Generate the caption\n",
    "caption = pipe(image)\n",
    "\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7f3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a8782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"C:/Users/AbdulRahman/Desktop/grad_folders/MAIN_GRAD/Travia/Implementation/AI/DataSet/lateset Dataset Main/pre_processed_df_without.csv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34a0742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['latitude', 'longitude', 'stars', 'review_count',\n",
       "       'attributes_RestaurantsPriceRange2', 'attributes_GoodForKids',\n",
       "       'attributes_Ambience_touristy', 'attributes_Ambience_romantic',\n",
       "       'attributes_Ambience_intimate', 'attributes_Ambience_trendy',\n",
       "       'attributes_Ambience_classy', 'attributes_Ambience_casual',\n",
       "       'Bars_Night', 'Beauty_Health_Care', 'Cafes', 'GYM',\n",
       "       'Restaurants_Cuisines', 'Shops'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7266d",
   "metadata": {},
   "source": [
    "# trying deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac19785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"microsoft/deberta-v3-large\", tokenizer=\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cba1a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.001462299027480185, 'token': 39528, 'token_str': 'handyman', 'sequence': 'The capital of France is handyman.'}, {'score': 0.0010288931662216783, 'token': 41499, 'token_str': 'ppt', 'sequence': 'The capital of France is ppt.'}, {'score': 0.0010131933959200978, 'token': 74048, 'token_str': 'Retail', 'sequence': 'The capital of France isRetail.'}, {'score': 0.0008667797665111721, 'token': 99424, 'token_str': 'gaffer', 'sequence': 'The capital of France is gaffer.'}, {'score': 0.0008372539305128157, 'token': 71733, 'token_str': 'trophy', 'sequence': 'The capital of France istrophy.'}]\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"The capital of France is [MASK].\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d8ad78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc8fc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d387c7c95514082b12ed41055a1dbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AbdulRahman\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf1039938c643f3bf521cfac3689f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58584a7e2794620bccf8b1b1b016967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930c8a0877754767906b082cca7bb87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8652905591945e08fe371eeb24e22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.41678985953330994, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141648977994919, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339266151189804, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.0444474071264267, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.03029717691242695, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "print(pipe(\"The capital of France is [MASK].\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7baedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\abdulrahman\\appdata\\roaming\\python\\python311\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\abdulrahman\\appdata\\roaming\\python\\python311\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdulrahman\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\abdulrahman\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abdulrahman\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ca02dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.26.4\n",
      "  Obtaining dependency information for numpy==1.26.4 from https://files.pythonhosted.org/packages/3f/6b/5610004206cf7f8e7ad91c5a85a8c71b2f2f8051a0c0c4d5916b76d6cbb2/numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/61.0 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/61.0 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/61.0 kB 375.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.0/61.0 kB 406.4 kB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/15.8 MB 1.5 MB/s eta 0:00:11\n",
      "   ---------------------------------------- 0.2/15.8 MB 2.0 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/15.8 MB 2.0 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/15.8 MB 2.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/15.8 MB 2.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.7/15.8 MB 2.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.9/15.8 MB 2.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.1/15.8 MB 2.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.2/15.8 MB 3.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.4/15.8 MB 3.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/15.8 MB 3.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.7/15.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.9/15.8 MB 3.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 3.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.2/15.8 MB 3.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.4/15.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.5/15.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.7/15.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.9/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.0/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.2/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.4/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.6/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.7/15.8 MB 3.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.9/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.2/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.4/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.6/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 4.8/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 4.9/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.1/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.3/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.4/15.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.6/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.8/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.9/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.1/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.3/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.5/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.6/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.8/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 7.0/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.2/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.3/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 7.5/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.6/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.8/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.9/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.0/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 8.2/15.8 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.3/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 8.4/15.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.9/15.8 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.0/15.8 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.1/15.8 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.2/15.8 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 9.6/15.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.8/15.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.9/15.8 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.9/15.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.5/15.8 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.7/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.8/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.9/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.4/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.6/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.9/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.1/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.2/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.3/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.4/15.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.6/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.9/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.2/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.4/15.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.7/15.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.1/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.2/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.3/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.4/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.6/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.0/15.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.3/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.4/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\AbdulRahman\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\AbdulRahman\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\~umpy.libs\\\\libscipy_openblas64_-caad452230ae4ddb57899b8b3a33c55c.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3234ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1064449",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74080dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc3c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb22b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d52f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The food was amazing and the ambience was cozy.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ae2d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  2833,  2001,  6429,  1998,  1996,  2572, 11283,  5897,\n",
       "          2001, 26931,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89e07d60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: barcelona's person controls the ball\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b2269a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a group of people sitting around a table playing instruments\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\AbdulRahman\\Desktop\\lol.jpg\"  # use raw string for Windows path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Preprocess image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "# Decode output\n",
    "caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca16ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
